{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "412b3522",
      "metadata": {},
      "source": [
        "### **CSV Automation**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58727ec6",
      "metadata": {},
      "source": [
        "#### **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5e6d377",
      "metadata": {},
      "source": [
        "Data import plays a pivotal role in data science. Knowing how to do it prevents the possibility of failure of the model. To achieve this, various methods, which differ depends on the data types (e.g. .csv, .txt, .json), are possible.\n",
        "\n",
        "Data acquisition is a large part of many data analytics projects and system development life cycles. This article will show you how to write a simple Python program that uses the BULK INSERT utility to rapidly insert data from a CSV file into a SQL Server database table."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aa37a04",
      "metadata": {},
      "source": [
        "#### **Import the Necessary Libraries** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ea53a35",
      "metadata": {
        "id": "0ea53a35"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import psycopg2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9db7ea77",
      "metadata": {
        "id": "9db7ea77"
      },
      "source": [
        "#### **Find and move the CSV files to new directory from working directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b9e7f2",
      "metadata": {
        "id": "f0b9e7f2"
      },
      "outputs": [],
      "source": [
        "#find csv files in the working directory\n",
        "\n",
        "csv_files = []\n",
        "for file in os.listdir(os.getcwd()):\n",
        "    if file.endswith('.csv'):\n",
        "        csv_files.append(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84b7905",
      "metadata": {
        "id": "d84b7905"
      },
      "outputs": [],
      "source": [
        "#make new directory\n",
        "\n",
        "dataset_dir = 'datasets'\n",
        "\n",
        "#create the bash command to make a new directory\n",
        "# mkdir dataset_dir\n",
        "\n",
        "try:\n",
        "    mkdir = 'mkdir {0}'.format(dataset_dir)\n",
        "    os.system(mkdir)\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9087941e",
      "metadata": {
        "id": "9087941e"
      },
      "outputs": [],
      "source": [
        "#move the CV files in the new directory\n",
        "\n",
        "#mv filename directory\n",
        "\n",
        "for csv in csv_files:\n",
        "    mv_file =\"mv '{0}' {1}\".format(csv, dataset_dir)\n",
        "    os.system(mv_file)\n",
        "    print(mv_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "180aab97",
      "metadata": {
        "id": "180aab97"
      },
      "source": [
        "#### **create Pandas DataFrame for CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2640b64e",
      "metadata": {
        "id": "2640b64e"
      },
      "outputs": [],
      "source": [
        "#Create Pandas dataframe for csv to clean the data\n",
        "\n",
        "data_path = os.getcwd()+'/'+dataset_dir+'/'\n",
        "\n",
        "df={}\n",
        "for file in csv_files:\n",
        "    try:\n",
        "        df[file] = pd.read_csv(data_path+file)\n",
        "    except UnicodeDecodeError:\n",
        "        df[file] = pd.read_csv(data_path+file, encoding=\"ISO-8859-1\")\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5638cfe5",
      "metadata": {
        "id": "5638cfe5"
      },
      "source": [
        "#### **Clean the table,columns name and Maps the Dtype of CSV file to Upload into the DataBase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97f6bbcf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "97f6bbcf",
        "outputId": "492437fe-6ccf-4205-963e-2fe138733cac"
      },
      "outputs": [],
      "source": [
        "# Clean the column, table name and maps the pandas datatype to sql datatype\n",
        "# Then upload the CSV file into the database\n",
        "\n",
        "for k in csv_files:\n",
        "    \n",
        "    dataframe = df[k]\n",
        "    \n",
        "    #clean table name\n",
        "    clean_tbl_name = k.lower().replace(\" \",\"_\").replace(\"?\",\"\")\\\n",
        "                      .replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\"%\",\"\")\\\n",
        "                      .replace(\")\",\"\").replace(r\"(\",\"\").replace(\"$\",\"\")\n",
        "    \n",
        "    \n",
        "    #remove .csv extension from clean_tbl_name\n",
        "    table_name='{0}'.format(clean_tbl_name.split('.')[0])\n",
        "    print(table_name)\n",
        "    \n",
        "    #clean columns name\n",
        "    dataframe.columns = [x.lower().replace(\" \",\"_\").replace(\"?\",\"\")\\\n",
        "                          .replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\"%\",\"\")\\\n",
        "                          .replace(\")\",\"\").replace(r\"(\",\"\").replace(\"$\",\"\") for x in dataframe.columns]\n",
        "    \n",
        "    #replacement dictionary that maps pandas dtypes to sql dtypes\n",
        "    replacements = {\n",
        "        'object':'varchar',\n",
        "        'float64':'float',\n",
        "        'int64':'int',\n",
        "        'datetime64':'timestamp',\n",
        "        'timedelta64[ns]':'varchar'\n",
        "    }\n",
        "    \n",
        "    #table schema\n",
        "    col_str=\",\".join(\"{} {}\".format(n,d) for (n,d) in zip(dataframe.columns,dataframe.dtypes.replace(replacements)))\n",
        "    print(col_str)\n",
        "    \n",
        "    #open database connection\n",
        "    host = 'project-db.******.ap-south-1.rds.amazonaws.com'\n",
        "    port = '5432'\n",
        "    dbname = 'postgres'\n",
        "    user = '*****'\n",
        "    password = '*******'\n",
        "    \n",
        "    #connect to database\n",
        "    conn_string = \"host=%s port=%s database=%s user=%s password=%s\" % (host, port, dbname, user, password)\n",
        "    conn = psycopg2.connect(conn_string)\n",
        "    cursor=conn.cursor()\n",
        "    print('Database opened succesfully')\n",
        "    \n",
        "    #drop table if exixsts\n",
        "    cursor.execute(\"drop table if exists %s\") % (table_name)\n",
        "    \n",
        "    #create table\n",
        "    cursor.execute(\"create table %s (%s)\") % (table_name, col_str)\n",
        "    print('{0} was created succesfully'.format(table_name))\n",
        "    \n",
        "    #insert values into the table\n",
        "    #save df to csv\n",
        "    dataframe.to_csv(k, header=dataframe.columns, index=False, encoding='utf-8')\n",
        "    \n",
        "    #open csv file into object\n",
        "    my_file = open(k)\n",
        "    print ('file opened in memory')\n",
        "    \n",
        "    #upload to db\n",
        "    SQL_STATEMENT = \"\"\"\n",
        "    COPY %s FROM STDIN WITH\n",
        "        CSV\n",
        "        HEADER\n",
        "        DELIMITER AS ','\n",
        "    \"\"\"\n",
        "    \n",
        "    cursor.copy_expert(sql=SQL_STATEMENT % table_name, file=my_file)\n",
        "    print ('file copied to db')\n",
        "    \n",
        "    cursor.execute(\"grant select on table %s to public\" % table_name)\n",
        "    conn.commit()\n",
        "    cursor.close()\n",
        "    print('table {0} imported to db completed'.format(table_name))\n",
        "    \n",
        "#for loop and message\n",
        "print ('all tables have been successfully imported into the db')\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "csv_automation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
